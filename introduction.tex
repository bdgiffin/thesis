\chapter{Introduction}

% OPENING (scope of the thesis)

	% A high-level description of the research setting:
		% Efficient & accurate approximation methods for nonlinear solid mechanics
	For decades, finite element methods (FEM) have been widely used by engineers and physicists in the modeling of solid continua. Numerous extensions of the method have been developed to model more complex physical processes, including large deformations and nonlinear material behavior.
	% A high% level statement of the fundamental problem(s):
		% FE solution accuracy is compromised by the issues of locking and mesh quality
		% Proposed solutions handle some, but not all of these problems
	In spite of these advances, traditional finite element methods have been plagued by recurrent issues of numerical accuracy pertaining to locking and poor mesh quality. Various strategies have been proposed to overcome some of these issues, though few have been able to address the underlying problem of element distortion sensitivity.
	
	% A high-level statement of the proposed solution(s):
		% Thesis presents a polytopal element framework to address these issues
		% Polyhedral discretization is aimed at overcoming issues of mesh quality
		% Framework is amenable to classical FE procedures used to address locking
	This thesis presents a novel polytopal element framework in an effort to address the aforementioned issues as a whole. The use of arbitrary polygonal and polyhedral shapes in place of canonical isoparametric elements seeks to resolve the issue of distortion sensitivity directly, obviating issues of meshing and mesh quality, while maintaining many of the desirable features of the FEM and its extensions.

\section{Historical Development} % (what has led up to polyhedral discretizations)

	% Origin of finite element methods, applied to solid mechanics problems
		% Finite elements developed initially for LINEAR, structural problems
	According to the historical account of Felippa \cite{Felippa:04}, finite element methods originated in the 1950s to address engineering challenges in, among other things, the design of aircraft. The method was subsequently given a more rigorous mathematical treatment by early contributors (Irons, Melosh, Strang), and its usage permeated to other fields of study (namely, structural engineering). Following the advent of the isoparametric element concept, displacement-based finite element formulations gained widespread popularity. Most early applications considered only small displacements and linear material behavior.
	% Subsequently, more complicated models were developed to accomodate nonlinearities:
		% Finite deformations incorporated to handle non-linear geometric effects
		% Advanced constitutive models developed for plastic and viscoelastic materials
		% Contact between deformable bodies (more geometric non-linearity)
	Subsequently, advanced solution methodologies were developed to accommodate various sources of nonlinearity, including finite deformations, nonlinear constitutive behavior, and contact.

	% Finite element methods still hold up in the aforementioned contexts because:
		% Compact support property of basis functions yields efficient solutions
			% Domain decomposition more straightforward, parallel scalability
		% Element quadrature rules effectively consider individual material points
			% leads to reasonable accuracy and solution efficiency
			% directly compatible with the theory of continuum mechanics
			% naturally accomodates non-linear kinematics
			% naturally accomodates non-linear constitutive models
		% Yields a precise description of mesh boundaries
			% allows for straightforward contact enforcement
			% application of other BC's (FSI pressure coupling) also straightforward
	Over half a century after its initial development, the finite element method is still widely used, and recognized as the industry standard technology for modeling complicated structural and dynamic systems. Its reigning popularity can be attributed to several desirable features of the method:
	\begin{itemize}
		\item The compact support property of the isoparametric basis functions helps to facilitate more efficient solution methodologies. In particular, the assembly of finite element systems of equations is rendered more efficient (and modular) by an element-wise assembly process.
		\item The Kronecker delta property and precise description of mesh boundaries allow for a relatively straightforward application of boundary conditions and contact constraints.
		\item Numerical quadrature on element domains derived from product Gauss rules balances accuracy, efficiency, and stability. Domain quadrature rules also naturally accommodate nonlinear kinematic behavior and ``black box'' constitutive models.
	\end{itemize}	
	
	% However, finite element methods have suffered from two major (but related) issues:
		% Poor solution accuracy due to effects of ``locking''
			% The quality and type of discretization heavily impacts accuracy
			% Low order elements suffer the most from these issues
			% Linear tetrahedra and triangles are the worst offenders
			% Thin an distorted elements perform poorly
		% Meshing of complex geometries into quality discretizations is difficult
			% Generally time-consuming to produce quality meshes (human effort)
			% Hard or impossible to avoid mesh distortion
			% Recent developments have been made towards hex-dominant meshing
				% still limited by element distortion
	However, despite these advantageous characteristics, finite element methods have suffered from two major (related) issues:
	\begin{itemize}
		\item[I.)] Standard element formulations are prone to the effects of numerical locking phenomena, which can significantly degrade the accuracy of the method. These issues are more prevalent for low-order elements, especially for linear triangles and tetrahedra. Moreover, very thin or distorted elements tend to exhibit more severe pathologies.
		\item[II.)] The process of discretizing complex geometries into traditional finite element shapes is not always possible with current automated meshing techniques. Contemporary meshing tools typically require extensive human intervention to produce meshes for complex shapes. This process is further encumbered by the aforementioned concerns over locking, to the extent that it becomes difficult -- if not impossible -- to produce quality discretizations within a reasonable amount of time.
	\end{itemize}
	
	Despite recent efforts to pursue automated quad-dominant \cite{Remacle:12} and hex-dominant \cite{Xifeng:17} meshing algorithms which seek to optimize certain mesh quality metrics, the inherent problem of element distortion sensitivity still remains. Consequently, substantial efforts have been made to address the locking problem through a variety of approaches. An overview of locking and its remedies is given in the following section.

	\subsection*{Locking in Finite Elements}
	
		% Establish fundamentally what locking is
			% observations regarding volumetric locking
			% observations regarding shear locking
			% mathematical characterization of locking by Babuska
		Locking, as a general phenomenon \cite{Babuska&Suri:92:1}, is characterized by a drastic loss of solution accuracy and/or convergence for particular choices of material or discretization parameters. In mathematical terms, an approximation method is deemed \textit{robust} (with respect to a given problem parameter) if its numerical solution converges ``uniformly'' to the exact solution under mesh refinement, for all values of the indicated problem parameter. Conversely, a method is said to exhibit \textit{locking} if the accuracy of the numerical solution degenerates as the chosen problem parameter approaches some limiting value.
		
		As a specific example, one of the most commonly discussed and addressed forms of locking in computational solid mechanics is the issue of \textit{volumetric locking}, wherein displacement-based element formulations suffer from a marked loss of accuracy when utilized to model the deformation of nearly incompressible materials. For an isotropic linear elastic material model, the parameter dependency in question relates to the Poisson's ratio of the material.
		
		Other forms of locking may manifest as a sensitivity to geometric/discretization parameters. These are collectively referred to as \textit{geometric locking} phenomena, which include: \textit{shear locking}, which is linked to the aspect ratio of continuum elements subjected to bending-dominated deformations; \textit{membrane locking}, which occurs in curved shell elements (see \cite{Winkler:10}); and \textit{trapezoidal locking}, which affects the bending response of distorted four-node quadrilateral elements (see \cite{MacNeal:87}).
	
		% Concerted efforts were made to address the issues of locking:
		
		% Higher-order elements
			% shown to eliminate locking issues at high enough order
			% accuracy still highly contingent on element distortion
			% complexity of high-order elements seen as disadvantageous
		Early efforts to address locking sought to develop more robust discretizations through the use of higher-order elements. In \cite{Babuska&Suri:92:2} and \cite{Suri:91}, Babu\u{s}ka and Suri rationalized the ability of high-order elements to overcome the effects of volumetric locking (for triangular discretizations). The improved convergence behavior of these elements made them an attractive option for seeking efficiency gains, as well. Nonetheless, higher-order elements were generally seen as too complex in comparison with the standard low-order elements commonly used for commercial applications. For example, many cite the relative difficulty of obtaining lumped mass matrices for high-order elements \cite{Hughes:00}. Moreover, the accuracy of high-order isoparametric elements can be severely degraded if the elements possess curved edges, as demonstrated in \cite{Lee&Bathe:93}.
			
		% Mixed formulations based on Hu-Washizu principle
			% one of the most robust solutions to locking
			% directed at incompressibility, resolves pressure field accurately
			% requires the interpolation of displacement, strain, and stress fields
			% must satisfy the inf-sup (LBB) conditions for stability
			% somewhat limited, based on the choice for the material model
			% can suffer from issues of numerical conditioning
		As an alternative to the standard displacement-based FEM, some have argued for the use of \textit{mixed finite element methods} (MFEM), which provide, in the most general case, a separate interpolation of the displacement, strain, and stress fields. Mixed methods are derived from a 3-field Hu-Washizu variational principle in the specification of the weak form. While these methods are generally less vulnerable to locking (as noted in \cite{Babuska&Suri:92:2} and \cite{Babuska&Suri:92:1}), they are not altogether immune to its effects. Moreover, mixed methods are subject to potential issues of stability, i.e. the Babu\u{s}ka-Brezzi -- or inf-sup -- conditions (\cite{Babuska:71}, \cite{Brezzi:74}.) Additionally, mixed methods tend to be less efficient in comparison with displacement-based FEM, and are not as flexible in their ability to handle arbitrary constitutive relationships.
		
		% Method of incompatible modes
			% aimed mostly at improving bending performance of elements
			% based on mixed formulations, reduced to 2-field method
			% considers enhanced strain mode degrees of freedom
			% enhanced dofs can be condensed out at the element level
			% performance of elements is still sensitive to mesh distortion
		In an effort to retain some of the beneficial characteristics of mixed finite element methods, \textit{(mixed) assumed strain methods} were formalized for geometrically linear problems in \cite{Simo&Rifai:90}, and extended to nonlinear problems in \cite{Simo&Armero:92} and \cite{Simo&Armero&Taylor:93}. Assumed strain methods are closely related to the \textit{method of incompatible modes} first proposed by Wilson \cite{Wilson:73}, which considers the inclusion of additional element-specific modes of deformation that allow for discontinuities in the resulting displacement field at element boundaries. By contrast, assumed strain methods are derived from a 3-field variational principle, and rely upon energy orthogonality between an enhanced strain field and the resulting stress field to eliminate the need for an independent stress interpolation space. Assumed strain methods may therefore be viewed as as a class of mixed 2-field formulations, involving a compatible displacement field and an enhanced/assumed strain field. Though the method is effective in treating a variety of geometric locking phenomena, it nonetheless leads to spurious instabilities in both linear and nonlinear problems (see \cite{Bathe&Sussman:14} and \cite{Bathe&Pantuso:97}, respectively), even for element formulations satisfying the inf-sup conditions.
			
		% Selective Reduced Integration techniques
			% developed to handle incompressibility constraint
			% works fairly well, though may fail inf-sup
		Other attempts to adapt mixed formulations for more general use with arbitrary material models without relying upon an explicit interpolation of the stress or strain fields include \textit{selective reduced integration} (SRI) and equivalent \textit{strain projection} techniques. These methods were fairly successful in overcoming the issues of volumetric locking via the B-bar projection approach for linear problems discussed in \cite{Hughes:00}, and via the F-bar approach for nonlinear problems discussed in \cite{Souza:96}. Though these methods can be both effective and efficient, their success has been limited to the problem of volumetric locking; they are somewhat less successful with regard to treating other forms of locking, such as shear locking in continuum finite elements \cite{Malkus&Hughes:78}. Also, SRI can lead to stability problems if the strain projection spaces are not selected carefully.
		
		% Hourglass stiffness and viscosity
			% use underintegration of the element (single point quadrature)
			% supplement with (small) artificial stiffness & viscosity parameters
			% efficient computationally, especially for explicit dynamics
			% an ad hoc solution, sensitive to the choice of HG parameters
		At the opposite end of the spectrum -- in recognition of the inherent issues of stability plaguing mixed methods and reduced integration techniques, methodologies employing \textit{orthogonal hourglass control} have been suggested \cite{Flanagan:81}. These approaches consider the use of low-order quadrature rules to avoid common locking phenomena, supplemented by artificial stiffness terms to maintain stability while preserving essential convergence characteristics. While these formulations tend to be highly efficient computationally (particularly for explicit dynamics), the obvious disadvantage of these approaches relates to their reliance upon user-specified artificial stiffness and viscosity parameters. Appropriate selection of these parameters is not always a trivial matter, and often warrants problem-specific investigations via sensitivity analyses.
			
		% Important point: most of these methods restricted to standard (hex) elements
			% not easily generalizable/extensible to other elements/methods
			% moreover, standard isoparametric elements are part of the problem
				% led to development of alternative methods/discretizations
		Ultimately, the foregoing methodologies have had only limited success in resolving the full spectrum of locking problems, particularly with regard to geometric locking phenomena. Moreover, many of the proposed enhancements are limited to specific element types (typically low-order quadrilaterals or hexahedra), and are not readily generalizable to other discretizations.
		
		% Say something about how accuracy is controlled by loss of completeness, integration consistency
		
		%To a large extent, the propensity for locking in FEM is controlled by the standard isoparametric elements' sensitivity to geometric distortion. In other words, model accuracy is too often the product of the chosen discretization. For this reason, considerable efforts have been invested in recent years towards the development of alternative discretizations and numerical methods, a few of which are discussed in the following section.
	
	\subsection*{Alternative Numerical Methods and Discretizations}
		% Efforts toward meshing & element quality led to new methods & discretizations
		Driven primarily by concerns over meshing (and additionally by concerns over element quality), a variety of alternative approximation methods have been explored in recent decades. %The general consensus is that the construction of an approximation space should be disconnected (to a variable extent) from the particular choice of discretization. In particular, isoparametric finite element basis functions are conveniently defined in terms of the discretization, but may not necessarily result in a suitable approximation space for the given problem at hand.
		
		% Say something about how we should like to define approximation spaces in a more flexible way. Additionally, consider the fact that these methods are intended/aimed at avoiding the meshing problem altogether. (Mesh-free methods, in particular, are driven by the desire to avoid having to create a mesh in the first place; not by element quality.)
		
		% Meshfree methods
			% avoid issues of mesh sensitivity altogether -- abandon the mesh
			% construct shape functions using weighting/kernal functions
			% several different varieties: SPH, MLS, RKPM, Max-Ent
			% solutions tend to be much smoother
			% able to handle severe/extreme deformations & changing topology
			% lose compact support property; less computationally efficient
			% boundaries are not well-defined; handling of BCs and contact is hard
			% suffers from issues of numerical integration; lose consistency
			% need to define a background mesh for integration (NOT mesh-free)
		In direct response to these considerations, so-called \textit{mesh-free} methods were developed in an effort to construct approximation spaces that are defined independently of a mesh. Mesh-free methods encompass a broad class of approximation schemes, including smoothed particle hydrodynamics (SPH), the element-free Galerkin (EFG) method, the natural element method (NEM), and the reproducing kernel particle method (RKPM), among others. A majority of these methods rely upon an \textit{a priori} specification of weighting functions used to construct an associated set of polynomially reproducing basis functions. The resulting mesh-free basis yields relatively smooth solutions which are less sensitive to the specific choice and distribution of weighting functions.
		
		% fix: specific choice and distribution of weighting functions
		
		The departure from defining an approximation space on a structured partition of the domain presents a number of challenges, however. Specifically, mesh-free methods still require the definition of a background mesh to effect numerical integration of the weak form, partially invalidating the namesake of the method. Moreover, because the mesh-free basis functions are typically non-polynomial in form (and because their supports are not disjoint on element domains), they are more challenging to accurately integrate. 

		% Discuss nodal integration methods, issues of stability, etc.
		
		% Shepard? Sherman?
		
		Chen et al. have proposed a means of overcoming these issues in \cite{Chen:13} through a \textit{variationally consistent integration} scheme, though this approach is susceptible to numerical instabilities. Additionally, because the basis functions do not in general exhibit the Kronecker delta property, the application of boundary conditions and contact constraints becomes less straightforward. Several techniques have been proposed to address this issue \cite{Mendez:04}, including Lagrange multiplier methods, Nitsche's method, and mesh blending at domain boundaries.
			
		% Discontinuous Galerkin methods
			% accomodate arbitrary polytopal domains
			% solution within a given polytopal domain is locally polynomial
			% tends to be fairly robust across different discretizations
			% relatively efficient, shares some favorable characteristics of FEM
			% penalty terms are needed to enforce BCs & weak continuity
			% penalty parameters are problem dependent, must be chosen carefully
		As another alternative, \textit{discontinuous Galerkin} (DG) methods have gained recent attention due to their more flexible representation of solution fields as piecewise polynomials over individual elements, resulting in solution discontinuities at element boundaries. To stabilize the resulting approximation space, DG methods supplement the weak form with interior penalty terms that seek to minimize these discontinuities. A detailed review of several primal interior penalty DG schemes may be found in \cite{Riviere:08}. DG methods are advantageous in that they may accommodate arbitrary element shapes, and exhibit desirable distortion robustness characteristics.
		
		DG approaches are not without their own problems, however. One commonly cited issue relates to the fact that DG methods can become sensitive to the choice of penalty parameters used to weakly enforce inter-element continuity and boundary conditions. Indeed, the selection of these parameters is problem dependent. Additionally, DG methods tend to suffer from poor numerical conditioning problems if the discretization does not conform to relatively stringent regularity requirements, thereby limiting the types of discretizations used with the method.
		
		% Add reference to regularity requirements of elements.
			
		% Weak Galerkin methods
			% more recently developed approach
			% can handle arbitrary polytopal element domains
			% based on the notion of weak derivatives
			% similar in some respects to DG methods; polynomial basis functions
			% dofs stored in element domains, and on element boundaries
			% weak derivative property avoids need for penalty terms (unlike DG)
			% may result in a large number of unknowns, less efficient
			% current formulations place limits on element geometry			
		
		The more recent \textit{weak Galerkin} (WG) finite element method first introduced by Wang and Ye for elliptic problems in \cite{Wang:13}, and for parabolic equations by Li and Wang in \cite{Li:13}, pursues the discretization of a (2D) problem domain into polygonal elements and element edges. The method considers a space of ``weak functions'' defined independently on element interiors and their boundaries, such that for any function $v = \left\{ v|_\Omega, \, v|_{\partial \Omega} \right\}$, its (discrete) ``weak gradient'' $\nabla_{w,k} v \in V(\Omega,k)$ is defined on the interior of a given element $\Omega \subset \mathbb{R}^d$ through its action on a vector field $\mathbf{q} \in V(\Omega,k)$:
\begin{equation}
	\int_\Omega \mathbf{q} \cdot \nabla_{w,k} v \, dA = - \int_\Omega v|_\Omega \nabla \cdot \mathbf{q} \, dA + \int_{\partial \Omega} v|_{\partial \Omega} \, \mathbf{q} \cdot \mathbf{n} \, dS, \quad \forall \mathbf{q} \in V(\Omega,k),
\end{equation}
where $V(K,k) \subset \left[ P^k (\Omega) \right]^d$ is a subspace of the vector-valued polynomials of maximal degree $k$ defined on $\Omega$. A weak Galerkin approximation method uses the discrete weak gradient operator in place of the classical gradient.

	The discrete approximation spaces under consideration are typically low-order polynomials (commonly just constants) defined on the elements and their edges. However, there are some non-trivial choices that must be made regarding an optimal selection of these polynomial spaces for the sake of computational efficiency (discussed in greater detail in reference \cite{Mu:15:1}). The method has so-far only been applied to the solution of linear problems.

	The computational accuracy of the weak Galerkin approach has been explored by Mu et. al. in \cite{Mu:13}, showing that for certain problems, the weak Galerkin method converges at rates comparable to those of the standard FEM. Lin et. al. performed a comparative study between the weak Galerkin, discontinuous Galerkin, and mixed finite element methods in \cite{Lin:15}, demonstrating some of the competitive and desirable characteristics of WG in contrast to DG or MFEMs: no need for penalty parameters, and definiteness of the resulting linear system of equations.

	Mu et. al. adapted the method for use on arbitrary polytopal meshes in \cite{Mu:15:2} and \cite{Mu:15:1}, albeit with a number of shape regularity restrictions placed on the elements. In general, the implementation of WG considers the mesh degrees of freedom as belonging to both the elements and their edges, however in \cite{Mu:15:1} it is noted that the local element degrees of freedom can be expressed in terms of the bounding edge degrees of freedom alone, to improve computational efficiency. A modified approach by Wang et. al. in \cite{Wang:14} instead chooses to express functions on edges only implicitly. Although this approach successfully eliminates the computational expense associated with edge-based degrees of freedom, it requires the inclusion of an additional interior penalty term in the weak form, similar to a discontinuous Galerkin method.

	% Polygonal and Polyhedral methods
		% element geometry made to be more flexible (arbitrary)
			% opens up new possibilities for meshing
		% construct basis functions directly on the elements (no isoparametric map)
		% basis functions possess compact support; numerically efficient
		% mesh boundaries again well-defined; good for contact & BC enforcement
		% techniques developed for efficient domain integration (Sukumar, Mousavi)
	In the wake of these investigations, it becomes clear that the combination of desirable properties held by the FEM (compact support, Kronecker delta, quadrature efficiency, DoF efficiency, stability, and inter-element compatibility) are qualities worth preserving. To this end, recent efforts within the numerical methods development community have focused on improving and generalizing existing element formulations for arbitrary element shapes -- polygons and polyhedra. Rather than relying upon shape functions defined through an isoparametric transformation from a parent element domain, recent polytopal element methodologies have explored various techniques for constructing approximants directly on physical element domains. In doing so, issues regarding distortion sensitivity of the elements are largely obviated, and new opportunities for discretizing the domain with irregular shapes are made possible.
	% New discretization methods have made polyhedral meshing more accessible
		% Voronoi meshing has made leaps and bounds in recent years (Ebeida)
		% Efforts made toward hex mesh intersection with B-rep geometry (Celeris)
	In like fashion, recent advances in meshing technologies have made polyhedral element methods a readily feasible option. Very recently, Ebeida et. al. have released VoroCrust \cite{Ebeida:17}: a robust voronoi discretization tool based on constrained Poisson disk sampling. Concurrent technologies have also advanced polyhedral mesh generation via boolean intersection of a background hexahedral mesh with a piecewise linear boundary representation (or B-rep).

	% Justify current demand for reliable and robust polyhedral element methods
	For these reasons, efficient and robust polyhedral element formulations are in high demand, leading to a proliferation of new approaches. A number of these are discussed in the following section.

\section{Recent Advances in Polytopal Element Methods} % (polyhedral finite element methods)

	% Reiterate the main motivation behind polyhedral discretizations
		% Want to avoid meshing limitations while preserving desirable FEM properties
			% definition of quadrature rules & precise boundary description
	Polytopal element methods combine many of the attractive features of FEM with the geometric flexibility afforded by arbitrary element shapes, the primary motivation for which arises from the aforementioned concerns over discretization sensitivity. Most of these methods share a few distinguishing features in common with one another: mesh degrees of freedom are borne by nodes -- the geometric vertices of (low-order) elements; nodal basis functions are compactly supported over adjoining element domains, and satisfy the Kronecker delta property; basis functions are defined directly on the element's physical domain, rather than on a parent domain.
	
	% Discuss meshing problem more!
	
	Where these methods differ is in the way that they choose to define an element's shape functions (if at all). These approaches may be loosely organized into three distinct categories: methods which explicitly define the element's basis functions via a continuous interpolation scheme; methods which define the element's shape functions only implicitly -- i.e. ``virtual'' element methods; and methods which form a discrete representation of the element's shape functions via an approximation scheme. These three methodologies are elaborated upon in the following sections.

	\subsection*{Continuous Interpolation on Arbitrary Polytopes}
		% Overview of efforts to define interpolants directly on polytopal domains
		A key advantage of isoparametric elements relates to their straight-forward, point-wise definition of an element's shape functions. In an effort to preserve this desirable characteristic, various authors have sought to explicitly define shape functions directly on arbitrary polytopal element domains. These efforts have led to the creation of a broad family of interpolation schemes, collectively referred to as \textit{generalized barycentric coordinates} \cite{Sukumar:17}.
		
		% Briefly describe the idea of generalized barycentric coordinates
			% harmonic/Laplace coordinates
			% Waschpress coordinates
			% maximum entropy coordinates
		At a minimum, shape functions which fall into this category must: form a partition of unity, satisfy linear completeness, and interpolate the nodal data (i.e. satisfy the Kronecker delta property). These coordinates are uniquely defined according to the standard barycentric coordinate system for simplicial domains. For arbitrary polytopal domains, numerous such coordinate systems exist, including Wachspress' coordinates \cite{Wachspress:75}, mean value coordinates \cite{Floater:03}, harmonic coordinates \cite{Joshi:07}, and maximum entropy coordinates \cite{Sukumar:04}, among others. In many cases, such coordinates are restricted to strictly convex polytopes.
			
		% Require significant number of quadrature points to retain accuracy
		Though generalized barycentric coordinates have been applied in the context of finite elements, their development has largely been propelled by the graphics community on account of their relatively smooth interpolatory properties. However, the smooth (non-polynomial) character of the shape functions presents a challenge with regard to accurate numerical integration. Consequently, relatively high-order quadrature schemes are required to achieve reasonable solution accuracy and satisfaction of patch tests. More recently, a polynomial projection scheme has been suggested in \cite{Talischi:14} to remedy these integration errors for linear problems; for general nonlinear problems, a gradient correction scheme has been proposed in \cite{Talischi:15} and \cite{Chi:16}. These developments have illuminated new possibilities for defining more efficient quadrature rules on polytopes, while still satisfying the essential requirements for convergence.
		
		% Many approaches are limited by restrictions on element geometry/convexity
		Yet in spite of these developments, many existing coordinate schemes are sill limited by moderate to severe restrictions on element shape/convexity, and produce sharp gradients in the presence of degenerate geometric features. These concerns, and a recognition of the fact that the shape functions need not be defined point-wise for finite element applications, have motivated research efforts toward discrete representations of element interpolants.

	\subsection*{Virtual Element Methods}
		% Overview of virtual element methods (avoids definition of SFs on element domains)
		% never explicitly constructs/represents shape functions on elements
		% considers ``virtual'' shape functions, projected onto a polynomial subspace
		% exploits linearity of bilinear form to separate consistency & stability terms
		% consistency term formed by low-order polynomial projection of virtual SFs
		% stability term formed by higher-order projection, scaled by HG parameter
		% limited in applicability to linear problems (some efforts toward nonlinear)
		% stabilization still a somewhat ad hoc procedure
		The \textit{virtual element method} (VEM) summarized in \cite{Veiga:13} is a relatively new approach, based in part upon the older concept of mimetic finite differences (MFD). Although the method supposes that continuous basis functions \textit{exist} within arbitrary polytopal elements, the VEM never explicitly \textit{defines} how these shape functions vary on element interiors. Instead, the VEM supposes that these ``virtual'' functions may be separated (via projection operators) into polynomial and non-polynomial parts, which are handled in different ways. The cornerstone of the method is the idea that the bilinear form for a given element may be decomposed into two distinct parts: a term which guarantees variational consistency (Galerkin exactness) involving the low-order polynomial part of the shape functions, and a term which provides stability involving the non-polynomial part. The consistency term must be integrated exactly, but the stability term can be evaluated approximately. Because this decomposition relies upon the linearity of the bilinear form, direct generalizations of the method to nonlinear problems are not immediately available.
		
		VEM has been applied to three-dimensional linear elasticity problems in \cite{Gain:13}. In \cite{Veiga:15}, a VEM formulation for low-order elements which accommodates nonlinear ``black-box'' constitutive algorithms is presented, and in \cite{Chi:17} an extension to finite deformations (with appropriate stabilization terms) is introduced. Given the means by which these approaches exploit the use of a projected uniform gradient to integrate the weak form, however, it is unclear how they could be extended to accommodate higher-order elements.
		
		Nonetheless, VEM formulations are able to tolerate geometric degeneracies and element non-convexity without encountering serious numerical difficulties, though their good behavior in the presence of these features is often governed by an ad hoc approximation of the stabilization terms. A more rigorous development of the corresponding stability terms for more complicated element domains remains to be explored.
		
	\subsection*{Approximate Interpolation on Arbitrary Polytopes}
		% Overview of efforts to define shape functions only loosely over element domains
		% Somewhere in between the barycentric coordinate-type elements, and the VEM
		In contrast with the previously described approaches, yet another strategy considers the representation of the element's shape functions in an approximate way, while enforcing a few essential requirements, namely: polynomial reproducibility, inter-element compatibility, and weak form consistency.
		
		% VETFEM (variable element topology finite element method)
			% define SFs as polynomials, subject to continuity requirements
			% suffers from robustness issues
		In \cite{Rashid:00} and \cite{Rashid:06}, Rashid and co-workers  explored the \textit{variable element topology finite element method} (VETFEM), characterized by an approximate representation of the element's shape functions as low-order polynomials satisfying weak continuity requirements at element boundaries. Within this framework, the element shape functions are determined by a local minimization problem, resulting in polynomial shape functions which optimize specified continuity and smoothness objectives. This minimization procedure is constrained by the requirements of consistency and reproducibility to guarantee satisfaction of linear patch tests. Dohrmann and Rashid later extended this approach to higher-order elements in \cite{Dohrmann:02}, instead focusing on a direct construction of shape function derivatives, rather than of the shape functions themselves.
		
		The VETFEM may be viewed as a non-conforming finite element method, as the minimization process altogether allows for residual discontinuities at inter-element boundaries. Nonetheless, because the elements satisfy weak continuity requirements, the method exhibits proper convergence characteristics. Additionally, because the VETFEM yields a point-wise representation of the shape functions as low-order polynomials, direct integration of the weak form can be carried out using relatively efficient domain quadrature rules.
			
		% DDPFEM (discrete data polyhedral finite element method)
			% define only SF values and gradients at discrete points
			% requires a partition of the element into cells
			% element formulation more robust with finer partition
		Although the VETFEM was developed to handle arbitrary polygonal elements, it was observed to suffer from sensitivity to geometric degeneracies and element non-convexity. In response to these issues, a \textit{discrete data polyhedral finite element method} (DDPFEM) proposed in \cite{Selimotic:08} was suggested to exploit the fact that for typical solid mechanics applications, it is generally only necessary to evaluate the element's shape functions (and their derivatives) at a discrete number of quadrature points. The proposed method shares several characteristics in common with the VETFEM, utilizing a similarly posed constrained minimization procedure to obtain the precise values and gradients of the shape functions at discrete points within a given element. One of the cited challenges with this approach pertains to the appropriate selection of an efficient quadrature rule for the elements.
			
		% PEM (partitioned element method)
			% embraces element partition idea
			% construct SFs in hierarchial manner, assume SFs piecewise polynomial
		Nonetheless, the initial thoughts put forward by the DDPFEM ultimately led to the development of the \textit{partitioned element method} (PEM) presented in \cite{Rashid:12}. The PEM proceeds by partitioning an element into polygonal quadrature cells, and allowing the element's shape functions to vary according to a local polynomial defined within each of these cells, resulting in piecewise polynomial shape functions which are discontinuous at quadrature cell boundaries. The polynomial coefficients defined in each cell are obtained by minimizing the discontinuities in the shape functions across all cell interfaces, subject to the necessary consistency and reproducibility constraints. The original presentation in \cite{Rashid:12} considers the shape functions to be approximate solutions of Laplace's equation on the partitioned element. Later developments have dispensed with this supposition, instead only penalizing discontinuities in the shape functions (and their gradients) at cell boundaries.
			
		% Joe Bishop's version of FE-based approximants w/ Laplace shape functions
			% similar in concept: discretize element domain
			% use FE basis functions on a local tet mesh, solve Laplace equation
		Subsequently, Bishop has proposed a very similar partitioned element scheme in \cite{Bishop:14}, wherein the elements and their faces are subdivided into simplices, resembling a local FE discretization of the element domain. The shape functions are obtained as the solution to Laplace's equation on this subdivision. Because this approach utilizes an FE-like discretization, the resulting shape functions are piecewise linear and $C^0$ continuous, thereby avoiding the need for a penalty term to enforce continuity. However, the method still requires the use of a gradient correction scheme to account for quadrature error and recover consistency with the weak form.

	% Lead into the present work, following last category: ``partitioned element methods''
		% select the PEM approach as the one of particular interest for this thesis
	In light of these developments, we choose to recognize a new class of methods, herein collectively referred to as \textit{partitioned element methods}. These may be viewed as a generalization of the original PEM presented in \cite{Rashid:12}. It is the subject of this thesis to further explore these methods, and to expose their particular merits and potential shortcomings.

\section{Scope of the Present Work}

	% Description of the partitioned element framework
		% Elements are discretized (partitioned) into cells (atomic units of mesh)
		% Approximation space is constructed on this partition (much like FE, etc.)
		% A local BVP is solved to obtain shape functions on the partition
		% Solution method depends on choice of local approximation method; many choices
		% Quadrature rules on the element are informed by the partition
	In this work, we put forth a general framework for partitioned element methods -- a collection of different approaches for constructing approximate representations of element shape functions on arbitrary polytopes. These methods are characterized by several distinguishing features:
	\begin{itemize}
		\item[(1)] Elements are discretized (partitioned) into non-overlapping polygonal cells. These cells are used to inform the quadrature rule of the element.
		\item[(2)] A local approximation space is defined on the partitioned element geometry.
		\item[(3)] A set of BVPs and appropriate constraints are posed, whose solutions yield the shape functions of the element.
		\item[(4)] A stable numerical scheme is developed to obtain approximate solutions to these BVPs using the approximation space defined in (2).
	\end{itemize}
	Numerous formulations are possible within this framework; a few of these are explored within the scope of this thesis.
	
	% Assert that nonlinearities (material & kinematic) are accomodated in this framework
	% Describe how this framework can accomodate standard anti% locking formulations
		% Allude to the incorporation of these methods as part of present research
	 The proposed application area of interest for these methods is in nonlinear solid mechanics. The efficacy of the methods explored will be assessed within this context, particularly with regard to their ability to naturally accommodate nonlinear kinematic and material behavior. The robustness of the resulting elements, and their performance in the face of locking phenomena will be evaluated. Additionally, several methodologies which leverage the partitioned element framework to combat certain forms of locking will be discussed.

	% Overview of contents each chapter
		% Chapter 2 establishes the computational solid mechanics BVP
		% Chapter 3 presents an accurate incremental kinematic update procedure (optional)
		% Chapter 4 presents a general class of partitioned element methods
		% Chapter 5 details an implementational framework for the PEM
		% Chapter 6 provides a number of numerical investigations into the PEM
		% Chapter 7 discusses opportunities for further research & development
	The remainder of this dissertation is organized as follows: chapter \ref{ch:solid_mechanics} establishes the context of nonlinear computational solid mechanics, chapter \ref{ch:pem} presents the overarching framework for partitioned element methods, chapter \ref{ch:implementation} details a particular implementational framework for the PEM, chapter \ref{ch:results} provides a number of numerical investigations of the PEM, and chapter \ref{ch:future_work} concludes with a discussion of opportunities for further research and development.